= Hardware specifications

Where did your run your tests? A laptop, or goldengate?
- I ran my tests on a personal laptop with ssh to goldengate.

What are the performance specifications of the machine: number and speed of
processors, size of RAM? (use `lscpu` and `free -m`)
- machine is running on x86_64 with 64 CPUs (32-bit and 64-bit). CPU ~4000 MHz, with a
    max of 4100 MHz and a min of 1200 MHz. Total amount of RAM is 95315 megabytes, with
    86823 megabytes free.

= Single-process mandelbrot

Run your executable, `single_mandelbrot`, with the following sizes and record
the amount of time it takes to compute each image.

[cols="1,1"]
!===
| Size | Time (s) 
| 100 | 0.01747
| 400 | 0.268261
| 800 | 1.06487
| 1000 | 1.6199
| 2000 | 6.49744
!===

= Multi-process mandelbrot

Run your executable, `multi_mandelbrot`, with the following sizes and record
the amount of time it takes to compute each image.

[cols="1,1"]
!===
| Size | Time (s) 
| 100 | 0.007749
| 400 | 0.138473
| 800 | 0.49523
| 1000 | 0.731681
| 2000 | 2.81732
!===


My results show that for larger computations, we can use multiple process to
break up the task into smaller pieces. We can use the fork() command to make
additional processes. The key part in this lab was to use shared memory so that
each process could compute information and store it in a way that is accessible
to the parent function once all the child processes finish. In this case,
splitting the job into 4 processes lead to computational efficiency improvements,
as we can see that the time savings increase as the image size increases.
Theoretically if we split a job amongst 4 children, we could see a 4x time savings.
In reality we see less than that because it takes time to assign tasks to multiple
processes, but this is miniscule compared to the savings we get in larger jobs.
